{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/travislatchman/TheHearts/blob/main/HeartSoundClassifier_CNN_1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0aMnZJYWXIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from scipy.signal import butter, lfilter\n",
        "import scipy.io\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import sklearn.metrics as metrics\n",
        "from torchsummary import summary\n",
        "\n",
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "rng = np.random.default_rng(seed) # seeded random number generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJGhLxvWn1T"
      },
      "outputs": [],
      "source": [
        "# # Define the CNN model\n",
        "# class HeartSoundClassifier(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(HeartSoundClassifier, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(1, 32, kernel_size=5)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.maxpool = nn.MaxPool1d(2)\n",
        "#         self.conv2 = nn.Conv1d(32, 64, kernel_size=5)\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc1 = nn.Linear(64 * ((X_train.shape[1] - 4) // 2 - 4) // 2, 128)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "#         self.fc2 = nn.Linear(128, 4)  # Assuming 4 classes\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.maxpool(self.relu(self.conv1(x)))\n",
        "#         x = self.maxpool(self.relu(self.conv2(x)))\n",
        "#         x = self.flatten(x)\n",
        "#         x = self.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhdlfT9PXGpI"
      },
      "outputs": [],
      "source": [
        "# # Instantiate the model\n",
        "# model = HeartSoundClassifier()\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "\n",
        "# # Define loss and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOQgLhtsXQRg"
      },
      "outputs": [],
      "source": [
        "# # Train the model\n",
        "# num_epochs = 20\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     for inputs, targets in train_loader:\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Validate the model\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in val_loader:\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += targets.size(0)\n",
        "#             correct += (predicted == targets).sum().item()\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Validation accuracy: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi7S0JCgXceb"
      },
      "outputs": [],
      "source": [
        "# # Final evaluation\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for inputs, targets in val_loader:\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = model(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += targets.size(0)\n",
        "#         correct += (predicted == targets).sum().item()\n",
        "\n",
        "# print(f'Final validation accuracy: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kycKlr4xKbR"
      },
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZvEYAZyxHwO"
      },
      "outputs": [],
      "source": [
        "# Preprocessing functions\n",
        "def load_audio_file(file_path, target_sample_rate = 2000):\n",
        "    audio_data, sample_rate = librosa.load(file_path, sr=target_sample_rate)\n",
        "    # if (sample_rate - target_sample_rate)**2 > 100:\n",
        "    #   audio_data, sample_rate = librosa.load(file_path, sr=2000)\n",
        "    return audio_data, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd8pDBNwxRWl"
      },
      "outputs": [],
      "source": [
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return b, a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUT1hc-yxTPt"
      },
      "outputs": [],
      "source": [
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpyPLUJGxVEf"
      },
      "outputs": [],
      "source": [
        "def apply_lowpass_filter(audio_data, sample_rate, cutoff=195):\n",
        "    filtered_data = butter_lowpass_filter(audio_data, cutoff, sample_rate)\n",
        "    return filtered_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1PwiHTXxXc1"
      },
      "outputs": [],
      "source": [
        "def perform_fft(filtered_data):\n",
        "    fft_data = np.fft.fft(filtered_data)\n",
        "    return fft_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFjcgThv1Gad"
      },
      "source": [
        "### Data Augmentation functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRqOFJe_1mYz"
      },
      "source": [
        "##### Time Shifting - shift the audio signal by a random amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-gK7f0V1KcC"
      },
      "outputs": [],
      "source": [
        "def time_shift(audio_data, sample_rate):\n",
        "    shift_amount = int(sample_rate * np.random.uniform(-0.1, 0.1))\n",
        "    shifted_data = np.roll(audio_data, shift_amount)\n",
        "    return shifted_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkji_fKx1tAc"
      },
      "source": [
        "##### Pitch Shifting - Change the pitch of the audio signal by a random factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPIdt85512Ab"
      },
      "outputs": [],
      "source": [
        "def pitch_shift(audio_data, sample_rate):\n",
        "    pitch_shift_amount = np.random.uniform(-1.0, 1.0)\n",
        "    shifted_data = librosa.effects.pitch_shift(audio_data, sr=sample_rate, n_steps=pitch_shift_amount)\n",
        "    return shifted_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC9egwA6165j"
      },
      "source": [
        "##### Add Random Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR6gGgYk19LW"
      },
      "outputs": [],
      "source": [
        "def add_noise(audio_data):\n",
        "    noise_level = np.random.uniform(0.005, 0.05)\n",
        "    noise = np.random.randn(len(audio_data)) * noise_level\n",
        "    noisy_data = audio_data + noise # add noise ~ normal(mean=0, std=noise_level)\n",
        "    return noisy_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl87moFrx4lt"
      },
      "source": [
        "### Building Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdNHHOeK2h2o"
      },
      "outputs": [],
      "source": [
        "class HeartSoundDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, num_output_samples, transform=None, weights=None, augmentation=False):\n",
        "        self.file_paths = file_paths\n",
        "        if torch.is_tensor(labels):\n",
        "          self.labels = labels\n",
        "        else:\n",
        "          self.labels = torch.from_numpy(labels).float()\n",
        "        self.transform = transform\n",
        "        self.num_output_samples = num_output_samples\n",
        "        self.weights = weights\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # file_path = self.file_paths[idx]\n",
        "        # audio_data, sample_rate = load_audio_file(file_path)\n",
        "        # file_paths is an array of all recordings\n",
        "        audio_data = self.file_paths[idx]\n",
        "        sample_rate = 2000\n",
        "        # Apply data augmentation techniques if the augmentation parameter is set to True\n",
        "        if self.augmentation:\n",
        "            audio_data = time_shift(audio_data, sample_rate)\n",
        "            audio_data = pitch_shift(audio_data, sample_rate)\n",
        "            audio_data = add_noise(audio_data)\n",
        "\n",
        "        filtered_data = apply_lowpass_filter(audio_data, sample_rate)\n",
        "        fft_data = perform_fft(filtered_data)\n",
        "\n",
        "        # Use only the real part of the FFT data\n",
        "        real_fft_data = np.real(fft_data)\n",
        "\n",
        "        # Normalize data\n",
        "        normalized_data = (real_fft_data - np.min(real_fft_data)) / (np.max(real_fft_data) - np.min(real_fft_data))\n",
        "\n",
        "        if self.transform:\n",
        "            normalized_data = self.transform(normalized_data)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.weights:\n",
        "          weight = self.weights[idx]\n",
        "          return torch.tensor(normalized_data[:self.num_output_samples]).float(), label, weight\n",
        "\n",
        "        return torch.tensor(normalized_data[:self.num_output_samples]).float(), label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8JWv0S23NX1"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(file_paths, labels, weights, max_num_samples, rng, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, batch_size=100, augmentation=True):\n",
        "    dataset_size = len(file_paths)\n",
        "    train_size = int(train_ratio * dataset_size)\n",
        "    val_size = int(val_ratio * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    dataset = HeartSoundDataset(file_paths, labels, max_num_samples)\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], generator=rng)\n",
        "\n",
        "    # Apply data augmentation only to the training dataset\n",
        "    train_set.dataset = HeartSoundDataset(train_set.dataset.file_paths, train_set.dataset.labels, \n",
        "                                          train_set.dataset.num_output_samples, weights=weights, augmentation=augmentation)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_1HYalyBFs"
      },
      "source": [
        "### Heart Sound Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t4fWpCFx99a"
      },
      "outputs": [],
      "source": [
        "# class HeartSoundClassifier(nn.Module):\n",
        "#     def __init__(self, input_size, num_classes):\n",
        "#         super(HeartSoundClassifier, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(1, 8, kernel_size=15, stride=1)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.pool1 = nn.MaxPool1d(8)\n",
        "#         self.conv2 = nn.Conv1d(8, 16, kernel_size=15, stride=1)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         self.pool2 = nn.MaxPool1d(8)\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         # self.fc1 = nn.Linear(input_size // 4 * 16, 32)\n",
        "#         self.fc1 = nn.Linear(960, 32)\n",
        "#         self.relu3 = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(32, num_classes)\n",
        "#         self.softmax = nn.Softmax(0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = self.pool1(self.relu1(self.conv1(x)))\n",
        "#         x = self.pool2(self.relu2(self.conv2(x)))\n",
        "#         x = self.flatten(x)\n",
        "#         x = self.relu3(self.fc1(x))\n",
        "#         # print(x.shape)\n",
        "#         x = self.fc2(x)\n",
        "#         # print(x.shape)\n",
        "#         return self.softmax(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q-2DnWtPf-M"
      },
      "source": [
        "### Classifier with Dropout and Regularization (REVIEW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCWVKuYPkmU"
      },
      "outputs": [],
      "source": [
        "class HeartSoundClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HeartSoundClassifier, self).__init__()\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # 1st Conv\n",
        "            nn.Conv1d(1, 8, kernel_size=19, stride=1),\n",
        "            nn.BatchNorm1d(8), # BATCH \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(16),\n",
        "            nn.Dropout(0.5), # DROPOUT\n",
        "        \n",
        "            # 2nd Conv\n",
        "            nn.Conv1d(8, 16, kernel_size=19, stride=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(16),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(528, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(32, 1) # Single logit to feed into Sigmoid\n",
        "            # self.softmax = nn.Softmax(0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.cnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxGqC-qO5nAR"
      },
      "outputs": [],
      "source": [
        "def write_prepped_audio(label_dir, file_name, audio_sample_len, target_sample_rate = 2000):\n",
        "  if file_name[-4:] != \".wav\": return None, None\n",
        "\n",
        "  if not os.path.exists(os.path.join(label_dir, \"prepped_audio\")):\n",
        "    os.makedirs(os.path.join(label_dir, \"prepped_audio\"))\n",
        "  new_file_path = os.path.join(label_dir, \"prepped_audio\", file_name)\n",
        "  if os.path.exists(new_file_path): return new_file_path, None\n",
        "\n",
        "  audio_data, sample_rate = load_audio_file(os.path.join(label_dir, file_name), target_sample_rate)\n",
        "\n",
        "  # if audio_data is shorter than 1/3 the target audio sample length, return None\n",
        "  # else, pad with zeros\n",
        "  if len(audio_data) < audio_sample_len/3:\n",
        "    return None, None\n",
        "  elif len(audio_data) < audio_sample_len:\n",
        "    audio_data = np.concatenate((audio_data, np.zeros(audio_sample_len)), dtype=np.float32)\n",
        "  scipy.io.wavfile.write(new_file_path, sample_rate, audio_data[:audio_sample_len])\n",
        "  return new_file_path, audio_data[:audio_sample_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru7SmQPFzG32"
      },
      "source": [
        "# Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keGkVuaGQqnH",
        "outputId": "f8c8c6cc-8da5-4ac2-f734-2f39458ed6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4RSLt6Yyk5S"
      },
      "outputs": [],
      "source": [
        "# Load data and create data loaders\n",
        "\n",
        "# data_dir = '/content/drive/MyDrive/MLMA Group/CardiacData/DatasetB'\n",
        "data_dir = '/content/drive/MyDrive/MLMA Group/CardiacData/physionet cardiac sounds/heart_sound/train'\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "audio_sample_len = 9000\n",
        "target_sample_rate = 2000\n",
        "data = []\n",
        "\n",
        "for label in os.listdir(data_dir):\n",
        "  label_dir = os.path.join(data_dir, label)\n",
        "  for file_name in os.listdir(label_dir):\n",
        "    new_file_path, _ = write_prepped_audio(label_dir, file_name, audio_sample_len, target_sample_rate)\n",
        "    if new_file_path is not None:\n",
        "      file_paths.append(new_file_path)\n",
        "      labels.append(label)\n",
        "      audio_data, _ = load_audio_file(new_file_path)\n",
        "      # Store all recordings into a list\n",
        "      data.append(audio_data[:audio_sample_len])\n",
        "data_array = np.vstack(data)\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Create weights for unbalanced dataset\n",
        "class_counts = dict(Counter(labels))\n",
        "class_weights = [class_counts['healthy']/len(labels), class_counts['unhealthy']/len(labels)]\n",
        "weights = list()\n",
        "for label in encoded_labels:\n",
        "  weights.append(class_weights[label])\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(data_array, encoded_labels, weights, audio_sample_len, rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH4mTenfOSkZ"
      },
      "outputs": [],
      "source": [
        "# file_lensB = np.zeros(len(file_paths))\n",
        "# for i, file_path in enumerate(file_paths):\n",
        "#   file_lensB[i] = len(load_audio_file(file_path)[0])\n",
        "\n",
        "# plt.hist(np.array(file_lensB)*0.5, bins=[0,1000,2000,3000,4000,5000,6000,7000,8000,9000,30000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjx7dG2ZP2vb"
      },
      "outputs": [],
      "source": [
        "# plt.hist(file_lens[:i], bins=[0,1000,2000,3000,4000,5000,6000,7000,8000,9000,50000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_UyDLT0y0Ou",
        "outputId": "26daeeaf-2907-483f-96fd-1bfc2a0cb88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1              [-1, 8, 8982]             160\n",
            "       BatchNorm1d-2              [-1, 8, 8982]              16\n",
            "              ReLU-3              [-1, 8, 8982]               0\n",
            "         MaxPool1d-4               [-1, 8, 561]               0\n",
            "           Dropout-5               [-1, 8, 561]               0\n",
            "            Conv1d-6              [-1, 16, 543]           2,448\n",
            "       BatchNorm1d-7              [-1, 16, 543]              32\n",
            "              ReLU-8              [-1, 16, 543]               0\n",
            "         MaxPool1d-9               [-1, 16, 33]               0\n",
            "          Dropout-10               [-1, 16, 33]               0\n",
            "          Flatten-11                  [-1, 528]               0\n",
            "           Linear-12                   [-1, 32]          16,928\n",
            "             ReLU-13                   [-1, 32]               0\n",
            "          Dropout-14                   [-1, 32]               0\n",
            "           Linear-15                    [-1, 1]              33\n",
            "================================================================\n",
            "Total params: 19,617\n",
            "Trainable params: 19,617\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 1.92\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 2.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = len(train_loader.dataset[0][0])\n",
        "num_classes = len(np.unique(encoded_labels))\n",
        "model = HeartSoundClassifier()\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "summary(model.cuda(), input_size=(9000,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBpmUbSPP9Lb"
      },
      "source": [
        "### Maybe add as a parameter for L1, L2 loss? (weight decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvGzwLmXQGET"
      },
      "outputs": [],
      "source": [
        "# Adjust this parameter for L1/L2 regularization\n",
        "weight_decay = 1e-5  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZV-8O26OuG"
      },
      "source": [
        "### Early Stopping (REVIEW) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSWPy27Z6Tou",
        "outputId": "054d48db-56c4-432e-bae7-bf2c249a1e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Training Loss: 0.1624, Training Accuracy: 79.37%, Validation Loss: 0.5004, Validation Accuracy: 80.25%\n",
            "Epoch [2/100], Training Loss: 0.1519, Training Accuracy: 79.37%, Validation Loss: 0.4783, Validation Accuracy: 80.25%\n",
            "Epoch [3/100], Training Loss: 0.1470, Training Accuracy: 79.37%, Validation Loss: 0.4689, Validation Accuracy: 80.25%\n",
            "Epoch [4/100], Training Loss: 0.1498, Training Accuracy: 79.37%, Validation Loss: 0.4545, Validation Accuracy: 80.25%\n",
            "Epoch [5/100], Training Loss: 0.1442, Training Accuracy: 79.37%, Validation Loss: 0.5081, Validation Accuracy: 80.25%\n",
            "Epoch [6/100], Training Loss: 0.1441, Training Accuracy: 79.37%, Validation Loss: 0.5266, Validation Accuracy: 80.25%\n",
            "Epoch [7/100], Training Loss: 0.1412, Training Accuracy: 79.37%, Validation Loss: 0.4512, Validation Accuracy: 80.25%\n",
            "Epoch [8/100], Training Loss: 0.1384, Training Accuracy: 79.37%, Validation Loss: 0.4438, Validation Accuracy: 80.25%\n",
            "Epoch [9/100], Training Loss: 0.1416, Training Accuracy: 79.41%, Validation Loss: 0.4586, Validation Accuracy: 80.25%\n",
            "Epoch [10/100], Training Loss: 0.1378, Training Accuracy: 79.50%, Validation Loss: 0.4891, Validation Accuracy: 80.25%\n",
            "Epoch [11/100], Training Loss: 0.1393, Training Accuracy: 79.37%, Validation Loss: 0.4532, Validation Accuracy: 80.25%\n",
            "Epoch [12/100], Training Loss: 0.1366, Training Accuracy: 79.37%, Validation Loss: 0.4843, Validation Accuracy: 80.25%\n",
            "Epoch [13/100], Training Loss: 0.1381, Training Accuracy: 79.37%, Validation Loss: 0.5209, Validation Accuracy: 80.25%\n",
            "Epoch [14/100], Training Loss: 0.1386, Training Accuracy: 79.37%, Validation Loss: 0.4492, Validation Accuracy: 80.25%\n",
            "Epoch [15/100], Training Loss: 0.1374, Training Accuracy: 79.37%, Validation Loss: 0.4670, Validation Accuracy: 80.25%\n",
            "Epoch [16/100], Training Loss: 0.1384, Training Accuracy: 79.37%, Validation Loss: 0.4584, Validation Accuracy: 80.25%\n",
            "Epoch [17/100], Training Loss: 0.1378, Training Accuracy: 79.37%, Validation Loss: 0.4552, Validation Accuracy: 80.25%\n",
            "Epoch [18/100], Training Loss: 0.1357, Training Accuracy: 79.37%, Validation Loss: 0.7469, Validation Accuracy: 80.25%\n",
            "Epoch [19/100], Training Loss: 0.1410, Training Accuracy: 79.37%, Validation Loss: 0.4676, Validation Accuracy: 80.25%\n",
            "Epoch [20/100], Training Loss: 0.1361, Training Accuracy: 79.37%, Validation Loss: 0.4643, Validation Accuracy: 80.25%\n",
            "Epoch [21/100], Training Loss: 0.1349, Training Accuracy: 79.37%, Validation Loss: 0.5578, Validation Accuracy: 80.25%\n",
            "Epoch [22/100], Training Loss: 0.1368, Training Accuracy: 79.37%, Validation Loss: 0.4606, Validation Accuracy: 80.25%\n",
            "Epoch [23/100], Training Loss: 0.1358, Training Accuracy: 79.37%, Validation Loss: 0.4503, Validation Accuracy: 80.25%\n",
            "Epoch [24/100], Training Loss: 0.1379, Training Accuracy: 79.37%, Validation Loss: 0.4698, Validation Accuracy: 80.25%\n",
            "Epoch [25/100], Training Loss: 0.1361, Training Accuracy: 79.37%, Validation Loss: 0.4669, Validation Accuracy: 80.25%\n",
            "Epoch [26/100], Training Loss: 0.1358, Training Accuracy: 79.37%, Validation Loss: 0.5054, Validation Accuracy: 80.25%\n",
            "Epoch [27/100], Training Loss: 0.1365, Training Accuracy: 79.37%, Validation Loss: 0.4728, Validation Accuracy: 80.25%\n",
            "Epoch [28/100], Training Loss: 0.1363, Training Accuracy: 79.37%, Validation Loss: 0.4461, Validation Accuracy: 80.25%\n",
            "Epoch [29/100], Training Loss: 0.1373, Training Accuracy: 79.37%, Validation Loss: 0.4550, Validation Accuracy: 80.25%\n",
            "Epoch [30/100], Training Loss: 0.1384, Training Accuracy: 79.32%, Validation Loss: 0.4566, Validation Accuracy: 80.25%\n",
            "Epoch [31/100], Training Loss: 0.1328, Training Accuracy: 79.37%, Validation Loss: 0.5068, Validation Accuracy: 80.25%\n",
            "Epoch [32/100], Training Loss: 0.1337, Training Accuracy: 79.41%, Validation Loss: 0.4696, Validation Accuracy: 80.25%\n",
            "Epoch [33/100], Training Loss: 0.1351, Training Accuracy: 79.37%, Validation Loss: 0.4812, Validation Accuracy: 80.25%\n",
            "Epoch [34/100], Training Loss: 0.1359, Training Accuracy: 79.32%, Validation Loss: 0.4662, Validation Accuracy: 80.25%\n",
            "Epoch [35/100], Training Loss: 0.1332, Training Accuracy: 79.37%, Validation Loss: 0.4453, Validation Accuracy: 80.25%\n",
            "Epoch [36/100], Training Loss: 0.1345, Training Accuracy: 79.54%, Validation Loss: 0.4478, Validation Accuracy: 80.25%\n",
            "Epoch [37/100], Training Loss: 0.1400, Training Accuracy: 79.59%, Validation Loss: 0.4599, Validation Accuracy: 80.25%\n",
            "Epoch [38/100], Training Loss: 0.1348, Training Accuracy: 79.41%, Validation Loss: 0.4552, Validation Accuracy: 80.25%\n",
            "Epoch [39/100], Training Loss: 0.1364, Training Accuracy: 79.37%, Validation Loss: 0.4681, Validation Accuracy: 80.25%\n",
            "Epoch [40/100], Training Loss: 0.1366, Training Accuracy: 79.37%, Validation Loss: 0.4886, Validation Accuracy: 80.25%\n",
            "Epoch [41/100], Training Loss: 0.1373, Training Accuracy: 79.37%, Validation Loss: 0.4727, Validation Accuracy: 80.25%\n",
            "Epoch [42/100], Training Loss: 0.1364, Training Accuracy: 79.32%, Validation Loss: 0.4931, Validation Accuracy: 80.25%\n",
            "Epoch [43/100], Training Loss: 0.1349, Training Accuracy: 79.37%, Validation Loss: 0.4710, Validation Accuracy: 80.25%\n",
            "Epoch [44/100], Training Loss: 0.1345, Training Accuracy: 79.37%, Validation Loss: 0.4890, Validation Accuracy: 80.25%\n",
            "Epoch [45/100], Training Loss: 0.1394, Training Accuracy: 79.54%, Validation Loss: 0.4792, Validation Accuracy: 80.25%\n",
            "Epoch [46/100], Training Loss: 0.1359, Training Accuracy: 79.37%, Validation Loss: 0.4794, Validation Accuracy: 80.25%\n",
            "Epoch [47/100], Training Loss: 0.1386, Training Accuracy: 79.37%, Validation Loss: 0.4583, Validation Accuracy: 80.25%\n",
            "Epoch [48/100], Training Loss: 0.1351, Training Accuracy: 79.41%, Validation Loss: 0.4631, Validation Accuracy: 80.25%\n",
            "Epoch [49/100], Training Loss: 0.1358, Training Accuracy: 79.37%, Validation Loss: 0.4510, Validation Accuracy: 80.25%\n",
            "Epoch [50/100], Training Loss: 0.1330, Training Accuracy: 79.32%, Validation Loss: 0.4659, Validation Accuracy: 80.25%\n",
            "Epoch [51/100], Training Loss: 0.1328, Training Accuracy: 79.54%, Validation Loss: 0.4479, Validation Accuracy: 80.25%\n",
            "Epoch [52/100], Training Loss: 0.1369, Training Accuracy: 79.59%, Validation Loss: 0.4925, Validation Accuracy: 80.25%\n",
            "Epoch [53/100], Training Loss: 0.1334, Training Accuracy: 79.37%, Validation Loss: 0.4513, Validation Accuracy: 80.25%\n",
            "Epoch [54/100], Training Loss: 0.1332, Training Accuracy: 80.07%, Validation Loss: 0.4548, Validation Accuracy: 80.25%\n",
            "Epoch [55/100], Training Loss: 0.1341, Training Accuracy: 79.41%, Validation Loss: 0.4729, Validation Accuracy: 80.25%\n",
            "Epoch [56/100], Training Loss: 0.1351, Training Accuracy: 79.32%, Validation Loss: 0.4696, Validation Accuracy: 80.25%\n",
            "Epoch [57/100], Training Loss: 0.1370, Training Accuracy: 79.28%, Validation Loss: 0.4655, Validation Accuracy: 80.25%\n",
            "Epoch [58/100], Training Loss: 0.1340, Training Accuracy: 79.41%, Validation Loss: 0.4667, Validation Accuracy: 80.25%\n",
            "Epoch [59/100], Training Loss: 0.1357, Training Accuracy: 79.23%, Validation Loss: 0.4632, Validation Accuracy: 80.25%\n",
            "Epoch [60/100], Training Loss: 0.1319, Training Accuracy: 79.41%, Validation Loss: 0.4556, Validation Accuracy: 80.25%\n",
            "Epoch [61/100], Training Loss: 0.1318, Training Accuracy: 79.76%, Validation Loss: 0.5101, Validation Accuracy: 80.25%\n",
            "Epoch [62/100], Training Loss: 0.1364, Training Accuracy: 79.37%, Validation Loss: 0.4680, Validation Accuracy: 80.25%\n",
            "Epoch [63/100], Training Loss: 0.1319, Training Accuracy: 79.37%, Validation Loss: 0.4410, Validation Accuracy: 80.25%\n",
            "Epoch [64/100], Training Loss: 0.1346, Training Accuracy: 79.32%, Validation Loss: 0.4556, Validation Accuracy: 80.25%\n",
            "Epoch [65/100], Training Loss: 0.1299, Training Accuracy: 79.50%, Validation Loss: 0.4831, Validation Accuracy: 80.25%\n",
            "Epoch [66/100], Training Loss: 0.1337, Training Accuracy: 79.41%, Validation Loss: 0.4531, Validation Accuracy: 80.25%\n",
            "Epoch [67/100], Training Loss: 0.1358, Training Accuracy: 79.41%, Validation Loss: 0.4866, Validation Accuracy: 80.25%\n",
            "Epoch [68/100], Training Loss: 0.1340, Training Accuracy: 79.41%, Validation Loss: 0.4488, Validation Accuracy: 80.25%\n",
            "Epoch [69/100], Training Loss: 0.1321, Training Accuracy: 79.41%, Validation Loss: 0.4692, Validation Accuracy: 80.25%\n",
            "Epoch [70/100], Training Loss: 0.1325, Training Accuracy: 79.89%, Validation Loss: 0.4672, Validation Accuracy: 80.25%\n",
            "Epoch [71/100], Training Loss: 0.1330, Training Accuracy: 79.32%, Validation Loss: 0.4520, Validation Accuracy: 80.25%\n",
            "Epoch [72/100], Training Loss: 0.1325, Training Accuracy: 79.41%, Validation Loss: 0.4644, Validation Accuracy: 80.25%\n",
            "Epoch [73/100], Training Loss: 0.1333, Training Accuracy: 79.67%, Validation Loss: 0.4895, Validation Accuracy: 80.25%\n",
            "Epoch [74/100], Training Loss: 0.1298, Training Accuracy: 79.37%, Validation Loss: 0.4540, Validation Accuracy: 80.25%\n",
            "Epoch [75/100], Training Loss: 0.1342, Training Accuracy: 79.67%, Validation Loss: 0.4769, Validation Accuracy: 80.25%\n",
            "Epoch [76/100], Training Loss: 0.1332, Training Accuracy: 79.72%, Validation Loss: 0.4541, Validation Accuracy: 80.25%\n",
            "Epoch [77/100], Training Loss: 0.1369, Training Accuracy: 79.37%, Validation Loss: 0.4493, Validation Accuracy: 80.25%\n",
            "Epoch [78/100], Training Loss: 0.1340, Training Accuracy: 79.54%, Validation Loss: 0.4931, Validation Accuracy: 80.25%\n",
            "Epoch [79/100], Training Loss: 0.1306, Training Accuracy: 79.32%, Validation Loss: 0.4788, Validation Accuracy: 80.25%\n",
            "Epoch [80/100], Training Loss: 0.1303, Training Accuracy: 79.63%, Validation Loss: 0.4723, Validation Accuracy: 80.25%\n",
            "Epoch [81/100], Training Loss: 0.1343, Training Accuracy: 79.59%, Validation Loss: 0.4995, Validation Accuracy: 80.25%\n",
            "Epoch [82/100], Training Loss: 0.1321, Training Accuracy: 79.59%, Validation Loss: 0.4894, Validation Accuracy: 80.25%\n",
            "Epoch [83/100], Training Loss: 0.1334, Training Accuracy: 79.50%, Validation Loss: 0.4443, Validation Accuracy: 80.25%\n",
            "Epoch [84/100], Training Loss: 0.1329, Training Accuracy: 79.81%, Validation Loss: 0.5195, Validation Accuracy: 80.25%\n",
            "Epoch [85/100], Training Loss: 0.1337, Training Accuracy: 79.89%, Validation Loss: 0.4900, Validation Accuracy: 80.25%\n",
            "Epoch [86/100], Training Loss: 0.1296, Training Accuracy: 79.50%, Validation Loss: 0.4535, Validation Accuracy: 80.25%\n",
            "Epoch [87/100], Training Loss: 0.1329, Training Accuracy: 79.50%, Validation Loss: 0.4286, Validation Accuracy: 80.25%\n",
            "Epoch [88/100], Training Loss: 0.1324, Training Accuracy: 80.20%, Validation Loss: 0.4724, Validation Accuracy: 80.25%\n",
            "Epoch [89/100], Training Loss: 0.1335, Training Accuracy: 79.32%, Validation Loss: 0.4491, Validation Accuracy: 80.25%\n",
            "Epoch [90/100], Training Loss: 0.1315, Training Accuracy: 79.67%, Validation Loss: 0.4530, Validation Accuracy: 80.25%\n",
            "Epoch [91/100], Training Loss: 0.1352, Training Accuracy: 79.76%, Validation Loss: 0.4221, Validation Accuracy: 80.25%\n",
            "Epoch [92/100], Training Loss: 0.1304, Training Accuracy: 79.41%, Validation Loss: 0.4474, Validation Accuracy: 80.25%\n",
            "Epoch [93/100], Training Loss: 0.1352, Training Accuracy: 79.54%, Validation Loss: 0.4997, Validation Accuracy: 80.25%\n",
            "Epoch [94/100], Training Loss: 0.1342, Training Accuracy: 79.45%, Validation Loss: 0.4673, Validation Accuracy: 80.25%\n",
            "Epoch [95/100], Training Loss: 0.1312, Training Accuracy: 79.81%, Validation Loss: 0.4669, Validation Accuracy: 80.25%\n",
            "Epoch [96/100], Training Loss: 0.1298, Training Accuracy: 79.85%, Validation Loss: 0.4738, Validation Accuracy: 80.25%\n",
            "Epoch [97/100], Training Loss: 0.1323, Training Accuracy: 79.32%, Validation Loss: 0.4599, Validation Accuracy: 80.25%\n",
            "Epoch [98/100], Training Loss: 0.1366, Training Accuracy: 79.32%, Validation Loss: 0.4702, Validation Accuracy: 80.25%\n",
            "Epoch [99/100], Training Loss: 0.1351, Training Accuracy: 79.32%, Validation Loss: 0.4650, Validation Accuracy: 80.25%\n",
            "Early stopping triggered after 99 epochs\n"
          ]
        }
      ],
      "source": [
        "# Set early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, classifier_threshold=0.5, patience=5, min_delta=0.001):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_accs = []\n",
        "  val_accs = []\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Initialize variables to track best validation loss and epochs without improvement\n",
        "  best_val_loss = 0\n",
        "  epochs_without_improvement = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    for batch_num, (data, labels, weights) in enumerate(train_loader):\n",
        "      data, labels, weights = data.to(device), labels.to(device), weights.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output_logits = model(data)[:,0]\n",
        "      intermediate_loss = criterion(output_logits, labels)\n",
        "      loss = torch.mean(weights*intermediate_loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * data.shape[0]\n",
        "      pred = nn.Sigmoid()(output_logits)\n",
        "      pred = torch.where(pred > classifier_threshold, 1, 0)\n",
        "      train_correct += (pred == labels).long().sum().item()\n",
        "\n",
        "    # Compute training accuracy and loss\n",
        "    train_acc = 100 * train_correct / len(train_loader.dataset)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, labels in val_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        output_logits = model(data)[:,0]\n",
        "        intermediate_loss = criterion(output_logits, labels)\n",
        "        loss = torch.mean(intermediate_loss)\n",
        "        val_loss += loss.item() * data.shape[0]\n",
        "        pred = nn.Sigmoid()(output_logits)\n",
        "        pred = torch.where(pred > classifier_threshold, 1, 0)\n",
        "        val_correct += (pred==labels).long().sum().item()\n",
        "\n",
        "    # Compute validation accuracy and loss\n",
        "    val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accs.append(val_acc)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if abs(val_loss - best_val_loss) > min_delta:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_without_improvement = 0\n",
        "    else:\n",
        "      epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      torch.save(model.state_dict(),'/content/drive/MyDrive/MLMA Group/physionet_model.pt')\n",
        "      return train_losses, train_accs, val_losses, val_accs\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/MLMA Group/physionet_model.pt')\n",
        "  \n",
        "  return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "train_losses, train_accs, val_losses, val_accs = train_model(model, train_loader, val_loader, optimizer, num_epochs, patience=patience, min_delta=min_delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UznsNzb5mUi4",
        "outputId": "0641d153-44ea-40c8-d30b-88804275ddee"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9f5e15eb3335>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (99,)"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
        "x = range(1,len(train_losses))\n",
        "ax[0].set_title('Training and Validation Loss')\n",
        "ax[0].plot(x, train_losses, label='Training')\n",
        "ax[0].plot(x, val_losses, label='Validation')\n",
        "ax[0].legend()\n",
        "ax[1].set_title('Training and Validation Accuracy')\n",
        "ax[1].plot(x, train_accs, label='Training')\n",
        "ax[1].plot(x, val_accs, label='Validation')\n",
        "ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSTHh8HA6VkL"
      },
      "source": [
        "### Train and Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zWha7uYly65X"
      },
      "outputs": [],
      "source": [
        "# # Train and validate the model\n",
        "# num_epochs = 10\n",
        "\n",
        "# train_losses = []\n",
        "# val_losses = []\n",
        "# train_accs = []\n",
        "# val_accs = []\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#   # Train\n",
        "#   model.train()\n",
        "#   train_loss = 0.0\n",
        "#   correct = 0\n",
        "#   total = 0\n",
        "#   for batch_num, (data, labels) in enumerate(train_loader):\n",
        "#     # print(f\"batch number {batch_num}\")\n",
        "#     optimizer.zero_grad()\n",
        "#     outputs = model(data)\n",
        "#     loss = criterion(outputs, labels)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     train_loss += loss.item()\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "\n",
        "#   # Compute training accuracy and loss\n",
        "#   train_acc = 100 * correct / total\n",
        "#   train_loss /= len(train_loader)\n",
        "#   train_accs.append(train_acc)\n",
        "#   train_losses.append(train_loss)\n",
        "\n",
        "#   # Validate\n",
        "#   model.eval()\n",
        "#   val_loss = 0.0\n",
        "#   val_correct = 0\n",
        "#   val_total = 0\n",
        "#   with torch.no_grad():\n",
        "#     for val_data, val_labels in val_loader:\n",
        "#       val_outputs = model(val_data)\n",
        "#       val_loss += criterion(val_outputs, val_labels).item()\n",
        "#       _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "#       val_total += val_labels.size(0)\n",
        "#       val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "#   # Compute validation accuracy and loss\n",
        "#   val_acc = 100 * val_correct / val_total\n",
        "#   val_loss /= len(val_loader)\n",
        "#   val_accs.append(val_acc)\n",
        "#   val_losses.append(val_loss)\n",
        "\n",
        "#   print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "# # Plot the loss functions\n",
        "# plt.plot(train_losses, label='Training Loss')\n",
        "# plt.plot(val_losses, label='Validation Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_p1mUc9smo4F"
      },
      "outputs": [],
      "source": [
        "def Test_model(model, test_dataloader, classifier_threshold=0.5):\n",
        "  \"\"\"Test model on test dataset and returns predictions and targets\"\"\"\n",
        "  model.eval()\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  target = []\n",
        "  prediction = []\n",
        "  with torch.no_grad():\n",
        "    for data, labels in test_dataloader:\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      output_logits = model(data)[:,0]\n",
        "\n",
        "      pred = nn.Sigmoid()(output_logits)\n",
        "      pred = torch.where(pred > classifier_threshold, 1, 0)\n",
        "      prediction.append(pred.cpu().tolist())\n",
        "      target.append(labels.cpu().tolist())\n",
        "  target = np.hstack(target)\n",
        "  prediction = np.hstack(prediction)\n",
        "\n",
        "  return target, prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V7lf_N-uy-zX"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model = HeartSoundClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/MLMA Group/physionet_model.pt'))\n",
        "y_true, y_pred = Test_model(model, test_loader)\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     y_true = []\n",
        "#     y_pred = []\n",
        "#     for data, labels in test_loader:\n",
        "#         outputs = model(data)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         y_true.extend(labels.numpy())\n",
        "#         y_pred.extend(predicted.numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2f0eOplQzA3I"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf0sO-7y8cuT"
      },
      "source": [
        "### F1, Precision, Recall, Sensitivity, Specificity, TP, FP, TN, FN, plot and ROC curve and calculate AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ZMueyLwm4ow"
      },
      "outputs": [],
      "source": [
        "recall = metrics.recall_score(y_true,y_pred)\n",
        "precision = metrics.precision_score(y_true,y_pred)\n",
        "accuracy = metrics.accuracy_score(y_true,y_pred)\n",
        "f1_score = metrics.f1_score(y_true,y_pred)\n",
        "print(f\"Test metrics:\\n\\tRecall-rate: {recall:.4f}\\n\\tPrecision: {precision:.4f}\"\n",
        "      +f\"\\n\\tAccuracy: {accuracy:.4f}\\n\\tF1-Score: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1kx7s1qPRy"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p9cHYUntqP4W"
      },
      "outputs": [],
      "source": [
        "# Load DatasetB\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/MLMA Group/CardiacData/DatasetB'\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "audio_sample_len = 9000\n",
        "target_sample_rate = 2000\n",
        "data = []\n",
        "\n",
        "for label in os.listdir(data_dir):\n",
        "  label_dir = os.path.join(data_dir, label)\n",
        "  for file_name in os.listdir(label_dir):\n",
        "    new_file_path, _ = write_prepped_audio(label_dir, file_name, audio_sample_len, target_sample_rate)\n",
        "    if new_file_path is not None:\n",
        "      file_paths.append(new_file_path)\n",
        "      labels.append(label)\n",
        "      audio_data, _ = load_audio_file(new_file_path)\n",
        "      # Store all recordings into a list\n",
        "      data.append(audio_data[:audio_sample_len])\n",
        "\n",
        "data_array = np.vstack(data)\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PtCP3PIDqWpN"
      },
      "outputs": [],
      "source": [
        "# Create weights for unbalanced dataset\n",
        "class_counts = dict(Counter(labels))\n",
        "classes = label_encoder.classes_\n",
        "class_weights = [class_counts[classes[0]]/len(labels), class_counts[classes[1]]/len(labels), class_counts[classes[2]]/len(labels)]\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(data_array, encoded_labels, audio_sample_len, rng, augmentation=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yVLVuEM1qbIy"
      },
      "outputs": [],
      "source": [
        "class PASCALClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_cnn):\n",
        "        super(PASCALClassifier, self).__init__()\n",
        "        self.cnn = pretrained_cnn\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(4*2247, 32),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.cnn(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FnZ1B1yPqdkB"
      },
      "outputs": [],
      "source": [
        "# Load pretrained model\n",
        "pretrained_model = HeartSoundClassifier()\n",
        "pretrained_model.load_state_dict(torch.load('/content/drive/MyDrive/MLMA Group/physionet_model.pt'))\n",
        "cnn = pretrained_model.cnn_layers\n",
        "\n",
        "# Freeze CNN layers\n",
        "for param in cnn.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Add new classification layers to tune\n",
        "PASCAL_model = PASCALClassifier(cnn)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "weights = torch.FloatTensor(class_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(PASCAL_model.classifier.parameters(), lr=0.0007, weight_decay=1e-5)\n",
        "summary(PASCAL_model.cuda(), input_size=(9000,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4yya1N0Pqvvd"
      },
      "outputs": [],
      "source": [
        "# Finetune classifier\n",
        "# Set early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.01\n",
        "num_epochs = 100\n",
        "\n",
        "def tune_model(model, criterion, train_loader, val_loader, optimizer, num_epochs=10, patience=5, min_delta=0.001):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_accs = []\n",
        "  val_accs = []\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Initialize variables to track best validation loss and epochs without improvement\n",
        "  best_val_loss = 0\n",
        "  epochs_without_improvement = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    for batch_num, (data, labels) in enumerate(train_loader):\n",
        "      labels = labels.type(torch.LongTensor)\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output_logits = model(data)\n",
        "      loss = criterion(output_logits, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * data.shape[0]\n",
        "      pred = torch.softmax(output_logits, dim=1)\n",
        "      pred = pred.argmax(dim=1)\n",
        "      train_correct += (pred == labels).long().sum().item()\n",
        "\n",
        "    # Compute training accuracy and loss\n",
        "    train_acc = 100 * train_correct / len(train_loader.dataset)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, labels in val_loader:\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        output_logits = model(data)\n",
        "        loss = criterion(output_logits, labels)\n",
        "        val_loss += loss.item() * data.shape[0]\n",
        "        pred = torch.softmax(output_logits, dim=1)\n",
        "        pred = pred.argmax(dim=1)\n",
        "        val_correct += (pred==labels).long().sum().item()\n",
        "\n",
        "    # Compute validation accuracy and loss\n",
        "    val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accs.append(val_acc)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    # Early stopping logic\n",
        "    if abs(val_loss - best_val_loss) > min_delta:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_without_improvement = 0\n",
        "    else:\n",
        "      epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      torch.save(model.state_dict(),'/content/drive/MyDrive/MLMA Group/PASCAL_model.pt')\n",
        "      return train_losses, train_accs, val_losses, val_accs\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/MLMA Group/PASCAL_model.pt')\n",
        "  \n",
        "  return train_losses, train_accs, val_losses, val_accs\n",
        "\n",
        "train_losses, train_accs, val_losses, val_accs = tune_model(PASCAL_model, criterion, train_loader, val_loader, optimizer, num_epochs, patience=patience, min_delta=min_delta)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}